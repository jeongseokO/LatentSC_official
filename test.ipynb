{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213b41fd",
   "metadata": {},
   "source": [
    "# Import Your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7d3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from utils.prompts import get_messages, get_cot_Ex\n",
    "# 0) 환경 설정\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "\n",
    "# 1) 모델·토크나이저 로드\n",
    "HF_REPO = \"YOUR MODEL\" # Huggingface repo name includes author's name. Cannot be provided on review stage.\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_REPO)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_REPO, \n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bf68e",
   "metadata": {},
   "source": [
    "# Response Generation with Summary Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25781896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.lsc_generate import add_enhanced_generation\n",
    "model = add_enhanced_generation(model)\n",
    "special_tokens = [tokenizer.encode(t, add_special_tokens=False)[0] for t in\n",
    "               [\"<Support1>\",\"<Support2>\",\"<Support3>\",\"<Support4>\",\"<Support5>\",\"<Support6>\",]]\n",
    "print(f\"Special IDs: {special_tokens}\")\n",
    "question = \"\"\"Let \\[f(x) = \\left\\{\n",
    "\\begin{array}{cl} ax+3, &\\text{ if }x>2, \\\\\n",
    "x-5 &\\text{ if } -2 \\le x \\le 2, \\\\\n",
    "2x-b &\\text{ if } x <-2.\n",
    "\\end{array}\n",
    "\\right.\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).\"\"\"\n",
    "\n",
    "# 이 함수들이 정의되어 있지 않으므로 주석 처리하고 대체 예시 제공\n",
    "cot_ex = get_cot_Ex(\"MATH\", \"llama3_8b\")\n",
    "model_name = \"llama3_8b\"\n",
    "\n",
    "# 예시 메시지\n",
    "messages = get_messages(\n",
    "    question=question,\n",
    "    cot_ex=cot_ex,\n",
    "    model_name=\"llama3_8b\",\n",
    "    dataset=\"MATH\"\n",
    ")\n",
    "\n",
    "# 1) Prepare prompt\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    enable_thinking=False\n",
    ").to(model.device)\n",
    "set_seed(42)\n",
    "\n",
    "result = model.generate_with_special_tokens(\n",
    "    inputs=inputs,\n",
    "    special_tokens=special_tokens,\n",
    "    remove_eos=True,\n",
    "    return_embeddings=True,\n",
    "    max_new_tokens=1024,\n",
    "    num_return_sequences=10,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"Lengths: {result.sequences.shape}\")\n",
    "\n",
    "seq_ids = []\n",
    "for seq in result.sequences:\n",
    "  sequence = seq.tolist()\n",
    "  seq_ids.append(sequence)\n",
    "prompt_len = inputs.shape[1]\n",
    "print(prompt_len)\n",
    "ans_list = []\n",
    "for id in seq_ids[:2]:\n",
    "  decoded = tokenizer.decode(\n",
    "      id[prompt_len:],\n",
    "      skip_special_tokens=False,\n",
    "      clean_up_tokenization_spaces=True\n",
    "  )\n",
    "  ans_list.append(decoded)\n",
    "  print(\"\\nDecoded text:\\n\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cacbbb",
   "metadata": {},
   "source": [
    "# LSC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "embs = result.mean_embeddings\n",
    "embs_norm = F.normalize(embs.float(), p=2, dim=1)\n",
    "sim_matrix = embs_norm @ embs_norm.T\n",
    "sim_np = sim_matrix.cpu().float().numpy()\n",
    "np.fill_diagonal(sim_np, 0.0)\n",
    "\n",
    "weights = np.exp(sim_np / 0.5)\n",
    "np.fill_diagonal(weights, 0.0)\n",
    "avg_weight = weights.mean(axis=1)\n",
    "best_idx = int(np.argmax(avg_weight))\n",
    "lsc_calib = avg_weight[best_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c931880",
   "metadata": {},
   "source": [
    "# Dynamic LSC Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_topk_lsc(weights, ans_list):\n",
    "    \"\"\"\n",
    "    Dynamic TopK LSC: Calculate from top2 to topN each,\n",
    "    find the point with the largest decrease in max_sim_score and use K right before that point\n",
    "    \"\"\"\n",
    "    num_paths = len(ans_list)\n",
    "    max_k = min(num_paths - 1, num_paths)  # Set maximum K\n",
    "    \n",
    "    max_scores = []\n",
    "    best_indices = []\n",
    "    avg_weights_list = []\n",
    "    \n",
    "    # Calculate from K=2 to max_k\n",
    "    for k in range(2, max_k + 1):\n",
    "        # Select only top K weights from each row and average\n",
    "        avg_weight_topk = np.array([\n",
    "            np.mean(np.sort(row)[-k:])\n",
    "            for row in weights\n",
    "        ])\n",
    "        \n",
    "        best_idx = int(np.argmax(avg_weight_topk))\n",
    "        max_score = avg_weight_topk[best_idx]\n",
    "        \n",
    "        avg_weights_list.append(avg_weight_topk)\n",
    "        max_scores.append(max_score)\n",
    "        best_indices.append(best_idx)\n",
    "    \n",
    "    # Find point with largest decrease in max_sim_score\n",
    "    optimal_k = max_k  # Default: last K\n",
    "    largest_drop = 0.0\n",
    "    largest_drop_idx = -1\n",
    "    \n",
    "    # Calculate differences between consecutive points to find largest decrease\n",
    "    for i in range(1, len(max_scores)):\n",
    "        score_diff = max_scores[i-1] - max_scores[i]  # Decrease from previous to current point\n",
    "        if score_diff > largest_drop:\n",
    "            largest_drop = score_diff\n",
    "            largest_drop_idx = i\n",
    "    \n",
    "    # Select K right before the point with largest decrease\n",
    "    if largest_drop_idx != -1 and largest_drop > 0:\n",
    "        optimal_k = largest_drop_idx + 1  # largest_drop_idx is where decrease occurred, so optimal is right before\n",
    "    \n",
    "    # Select result corresponding to optimal_k\n",
    "    k_idx = optimal_k - 2  # Convert to 0-based index (K=2 is index 0)\n",
    "    if k_idx >= len(best_indices):\n",
    "        k_idx = len(best_indices) - 1\n",
    "    elif k_idx < 0:\n",
    "        k_idx = 0\n",
    "    \n",
    "    selected_idx = best_indices[k_idx]\n",
    "    selected_answer = ans_list[selected_idx]\n",
    "    selected_conf = ans_list.count(selected_answer) / len(ans_list)\n",
    "    selected_calib = avg_weights_list[k_idx][selected_idx]\n",
    "    \n",
    "    # Add decrease information\n",
    "    score_diffs = []\n",
    "    for i in range(1, len(max_scores)):\n",
    "        score_diffs.append(max_scores[i-1] - max_scores[i])\n",
    "    \n",
    "    return {\n",
    "        \"answer\": selected_answer,\n",
    "        \"optimal_k\": optimal_k,\n",
    "        \"selected_idx\": selected_idx,\n",
    "        \"conf\": selected_conf,\n",
    "        \"calib\": selected_calib,\n",
    "        \"max_scores\": max_scores,\n",
    "        \"score_diffs\": score_diffs,\n",
    "        \"largest_drop\": largest_drop,\n",
    "        \"largest_drop_idx\": largest_drop_idx,\n",
    "        \"all_k_results\": list(zip(range(2, max_k + 1), max_scores, best_indices))\n",
    "    }\n",
    "\n",
    "result = dynamic_topk_lsc(weights, ans_list)\n",
    "print(f\"Selected Answer: {result['answer']}\")\n",
    "print(f\"Optimal K: {result['optimal_k']}\")\n",
    "print(f\"Selected Index: {result['selected_idx']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f450827",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
